{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SKFzZ2qVXPwa"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers accelerate ninja"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "    raise SystemError(\"GPU NOT FOUND\")\n",
        "\n",
        "gpu_name = torch.cuda.get_device_name(0)\n",
        "compute_capability = torch.cuda.get_device_capability(0)\n",
        "print(f\"GPU Name: {gpu_name}  (compute capability {compute_capability})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "QHfvmqNfXTxb",
        "outputId": "d88fd137-81bb-47ad-b016-8ffa7e7b5126"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU Name: Tesla T4  (compute capability (7, 5))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
        "\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "DTYPE      = torch.float16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = (\n",
        "    AutoModelForMaskedLM\n",
        "    .from_pretrained(MODEL_NAME, torch_dtype=DTYPE)\n",
        "    .to(\"cuda\")\n",
        "    .eval()\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CnVabJfUXzLH",
        "outputId": "ad49cca6-f459-4651-9935-3e66203c2f04"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_text = \"hello world\"\n",
        "inputs = tokenizer(example_text, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits  = outputs.logits\n",
        "\n",
        "print(\"Logits tensor shape:\", logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a8QCiuvuYF7t",
        "outputId": "45c2a759-0763-4327-9a73-8b93e10eee42"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logits tensor shape: torch.Size([1, 4, 30522])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Profiling"
      ],
      "metadata": {
        "id": "BmYjKKTobrBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.profiler import profile, record_function, ProfilerActivity\n",
        "\n",
        "def run_profiler(model, tokenizer, text_batch, training=False, warmup_iters=2, row_limit=20):\n",
        "\n",
        "    device = next(model.parameters()).device\n",
        "    model.train() if training else model.eval()\n",
        "\n",
        "    batch = tokenizer(text_batch, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    # 1. Warm-up (kept out of the trace)\n",
        "    for _ in range(warmup_iters):\n",
        "        if training:\n",
        "            out = model(**batch, labels=batch[\"input_ids\"])\n",
        "            out.loss.backward()\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                _ = model(**batch)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    # 2. One profiled step\n",
        "    tag       = \"training\" if training else \"inference\"\n",
        "    tracefile = f\"trace_{tag}.json\"\n",
        "\n",
        "    with profile(activities=[\n",
        "                    ProfilerActivity.CPU,\n",
        "                    ProfilerActivity.CUDA\n",
        "                 ],\n",
        "                 record_shapes=True,\n",
        "                 profile_memory=True,\n",
        "                 with_stack=True) as prof:\n",
        "        with record_function(tag):\n",
        "            if training:\n",
        "                out = model(**batch, labels=batch[\"input_ids\"])\n",
        "                out.loss.backward()\n",
        "            else:\n",
        "                with torch.no_grad():\n",
        "                    _ = model(**batch)\n",
        "\n",
        "\n",
        "    print(f\"Top {row_limit} CUDA kernels ({tag}):\")\n",
        "    print(prof.key_averages().table(sort_by=\"self_cuda_time_total\", row_limit=row_limit))\n",
        "\n",
        "    prof.export_chrome_trace(tracefile)\n",
        "    print(f\"Full timeline saved to ./{tracefile}\")"
      ],
      "metadata": {
        "id": "yVEWBEOQYYz5"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"the quick brown fox jumps over the lazy dog\"] * 8\n",
        "\n",
        "run_profiler(model, tokenizer, sentences, training=False)  # inference trace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cENz9Gq0c4Np",
        "outputId": "35c74003-6e71-43b0-b6ba-0f41cbc61464"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 CUDA kernels (inference):\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                              inference         0.00%       0.000us         0.00%       0.000us       0.000us      25.335ms       521.93%      25.335ms      25.335ms           0 b           0 b           0 b           0 b             1  \n",
            "                                            aten::addmm        14.10%       4.032ms        21.62%       6.182ms      83.546us       4.028ms        82.98%       4.028ms      54.429us           0 b           0 b      19.17 Mb     -54.83 Mb            74  \n",
            "         turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.052ms        42.27%       2.052ms      33.635us           0 b           0 b           0 b           0 b            61  \n",
            "    turing_fp16_s1688gemm_fp16_128x128_ldg8_relu_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us     817.378us        16.84%     817.378us      68.115us           0 b           0 b           0 b           0 b            12  \n",
            "void cutlass::Kernel2<cutlass_75_tensorop_f16_s1688g...         0.00%       0.000us         0.00%       0.000us       0.000us     607.116us        12.51%     607.116us     607.116us           0 b           0 b           0 b           0 b             1  \n",
            "void cublasLt::splitKreduce_kernel<32, 16, int, __ha...         0.00%       0.000us         0.00%       0.000us       0.000us     551.537us        11.36%     551.537us       9.042us           0 b           0 b           0 b           0 b            61  \n",
            "                     aten::_efficient_attention_forward         1.06%     302.718us         2.49%     710.995us      59.250us     313.525us         6.46%     313.525us      26.127us         192 b           0 b       1.55 Mb           0 b            12  \n",
            "fmha_cutlassF_f16_aligned_64x64_rf_sm75(PyTorchMemEf...         0.00%       0.000us         0.00%       0.000us       0.000us     313.525us         6.46%     313.525us      26.127us           0 b           0 b           0 b           0 b            12  \n",
            "                                aten::native_layer_norm         2.96%     846.083us         6.27%       1.793ms      68.958us     258.651us         5.33%     258.651us       9.948us           0 b           0 b       3.38 Mb           0 b            26  \n",
            "void at::native::(anonymous namespace)::vectorized_l...         0.00%       0.000us         0.00%       0.000us       0.000us     258.651us         5.33%     258.651us       9.948us           0 b           0 b           0 b           0 b            26  \n",
            "                                              aten::add         2.11%     604.622us         3.19%     913.217us      36.529us      98.046us         2.02%      98.046us       3.922us           0 b           0 b       3.22 Mb       3.22 Mb            25  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      98.046us         2.02%      98.046us       3.922us           0 b           0 b           0 b           0 b            25  \n",
            "                                             aten::gelu         1.03%     293.984us         1.59%     453.356us      34.874us      97.979us         2.02%      97.979us       7.537us           0 b           0 b       6.32 Mb       6.32 Mb            13  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us      97.979us         2.02%      97.979us       7.537us           0 b           0 b           0 b           0 b            13  \n",
            "                                     aten::index_select         0.63%     178.774us         1.28%     364.766us     121.589us      35.166us         0.72%      35.166us      11.722us           0 b           0 b     280.50 Kb           0 b             3  \n",
            "void at::native::(anonymous namespace)::indexSelectS...         0.00%       0.000us         0.00%       0.000us       0.000us      18.463us         0.38%      18.463us      18.463us           0 b           0 b           0 b           0 b             1  \n",
            "void at::native::(anonymous namespace)::indexSelectL...         0.00%       0.000us         0.00%       0.000us       0.000us      16.703us         0.34%      16.703us       8.351us           0 b           0 b           0 b           0 b             2  \n",
            "                                              aten::all         0.17%      49.435us         0.25%      70.465us      70.465us      10.687us         0.22%      10.687us      10.687us           0 b           0 b         512 b         512 b             1  \n",
            "void at::native::reduce_kernel<512, 1, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us      10.687us         0.22%      10.687us      10.687us           0 b           0 b           0 b           0 b             1  \n",
            "                                             aten::add_         0.10%      29.915us         0.16%      46.026us      46.026us       6.720us         0.14%       6.720us       6.720us           0 b           0 b           0 b           0 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 28.601ms\n",
            "Self CUDA time total: 4.854ms\n",
            "\n",
            "Full timeline saved to ./trace_inference.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"the quick brown fox jumps over the lazy dog\"] * 8\n",
        "run_profiler(model, tokenizer, sentences, training=True)   # training trace"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EdQD9UJ1c5Oa",
        "outputId": "c1fa6e4d-0847-4675-9435-97c4ac06e100"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 20 CUDA kernels (training):\n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                                   Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "                                               training         0.00%       0.000us         0.00%       0.000us       0.000us      22.869ms       119.54%      22.869ms      22.869ms           0 b           0 b           0 b           0 b             1  \n",
            "                                               aten::mm         4.83%       4.688ms         6.78%       6.573ms      44.409us       6.737ms        35.21%       6.737ms      45.518us           0 b           0 b     230.10 Mb     230.10 Mb           148  \n",
            "                                            aten::addmm         3.74%       3.626ms         5.48%       5.315ms      71.824us       4.039ms        21.11%       4.039ms      54.578us           0 b           0 b      19.17 Mb     -54.83 Mb            74  \n",
            "void at::native::vectorized_elementwise_kernel<4, at...         0.00%       0.000us         0.00%       0.000us       0.000us       3.791ms        19.82%       3.791ms      13.736us           0 b           0 b           0 b           0 b           276  \n",
            "                                             aten::add_         2.11%       2.047ms         4.22%       4.096ms      16.319us       3.166ms        16.55%       3.166ms      12.615us           0 b           0 b           0 b           0 b           251  \n",
            "         turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us       2.049ms        10.71%       2.049ms      33.592us           0 b           0 b           0 b           0 b            61  \n",
            "turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us       1.667ms         8.71%       1.667ms      27.324us           0 b           0 b           0 b           0 b            61  \n",
            "turing_fp16_s1688gemm_fp16_256x64_ldg8_f2f_stages_32...         0.00%       0.000us         0.00%       0.000us       0.000us       1.341ms         7.01%       1.341ms      27.370us           0 b           0 b           0 b           0 b            49  \n",
            "                                              aten::sum         1.70%       1.652ms         2.44%       2.369ms      31.592us       1.316ms         6.88%       1.316ms      17.552us           0 b           0 b     240.00 Kb     240.00 Kb            75  \n",
            "void at::native::reduce_kernel<128, 4, at::native::R...         0.00%       0.000us         0.00%       0.000us       0.000us       1.277ms         6.67%       1.277ms      17.253us           0 b           0 b           0 b           0 b            74  \n",
            "                    aten::_efficient_attention_backward         0.38%     372.279us         1.01%     980.785us      81.732us     943.237us         4.93%     943.237us      78.603us           0 b           0 b       4.64 Mb     -18.07 Mb            12  \n",
            "fmha_cutlassB_f16_aligned_64x64_k64_dropout_sm75(PyT...         0.00%       0.000us         0.00%       0.000us       0.000us     943.237us         4.93%     943.237us      78.603us           0 b           0 b           0 b           0 b            12  \n",
            "    turing_fp16_s1688gemm_fp16_128x128_ldg8_relu_f2f_tn         0.00%       0.000us         0.00%       0.000us       0.000us     830.889us         4.34%     830.889us      69.241us           0 b           0 b           0 b           0 b            12  \n",
            "turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     754.920us         3.95%     754.920us      62.910us           0 b           0 b           0 b           0 b            12  \n",
            "         turing_fp16_s1688gemm_fp16_128x128_ldg8_f2f_nt         0.00%       0.000us         0.00%       0.000us       0.000us     680.748us         3.56%     680.748us      56.729us           0 b           0 b           0 b           0 b            12  \n",
            "void cutlass::Kernel2<cutlass_75_tensorop_f16_s1688g...         0.00%       0.000us         0.00%       0.000us       0.000us     647.757us         3.39%     647.757us     647.757us           0 b           0 b           0 b           0 b             1  \n",
            "                                              aten::add         0.49%     475.606us         0.69%     672.917us      25.881us     631.405us         3.30%     631.405us      24.285us           0 b           0 b      47.93 Mb      47.93 Mb            26  \n",
            "                       aten::native_layer_norm_backward         0.50%     480.116us         1.43%       1.388ms      53.374us     623.790us         3.26%     623.790us      23.992us           0 b           0 b       3.43 Mb           0 b            26  \n",
            "turing_fp16_s1688gemm_fp16_256x128_ldg8_f2f_stages_3...         0.00%       0.000us         0.00%       0.000us       0.000us     621.839us         3.25%     621.839us      51.820us           0 b           0 b           0 b           0 b            12  \n",
            "void cutlass::Kernel2<cutlass_75_tensorop_f16_s1688g...         0.00%       0.000us         0.00%       0.000us       0.000us     606.414us         3.17%     606.414us     606.414us           0 b           0 b           0 b           0 b             1  \n",
            "-------------------------------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
            "Self CPU time total: 96.973ms\n",
            "Self CUDA time total: 19.131ms\n",
            "\n",
            "Full timeline saved to ./trace_training.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example payload: 8 sentences, padded to same length\n",
        "batch_txt = [\n",
        "    \"the quick brown fox jumps over the lazy dog\",\n",
        "] * 8                                           # batch size = 8\n",
        "\n",
        "# Tokenize → tensors → push to CUDA\n",
        "batch = tokenizer(\n",
        "    batch_txt,\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ").to(\"cuda\")      # batch is a dict with input_ids, attention_mask, etc."
      ],
      "metadata": {
        "id": "poP5ocs70ffD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, gc, statistics as stats, time\n",
        "\n",
        "def _cuda_timer(step_fn, warm=50, iters=500):\n",
        "    \"\"\"\n",
        "    step_fn() must run exactly ONE forward or train step.\n",
        "    Returns median and stdev (in ms) across `iters` timed runs.\n",
        "    \"\"\"\n",
        "    torch.backends.cuda.matmul.allow_tf32 = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "    gc.disable()\n",
        "\n",
        "    for _ in range(warm):\n",
        "        step_fn()\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "    start = torch.cuda.Event(enable_timing=True)\n",
        "    end   = torch.cuda.Event(enable_timing=True)\n",
        "    times = []\n",
        "\n",
        "    for _ in range(iters):\n",
        "        start.record()\n",
        "        step_fn()\n",
        "        end.record()\n",
        "        torch.cuda.synchronize()          # wait for kernel to finish\n",
        "        times.append(start.elapsed_time(end))  # ms\n",
        "\n",
        "    return stats.median(times), stats.stdev(times)\n",
        "\n",
        "@torch.inference_mode()\n",
        "def _fwd_step():\n",
        "    _ = model(**batch)\n",
        "\n",
        "def _train_step():\n",
        "    loss = model(**batch, labels=batch[\"input_ids\"]).loss\n",
        "    loss.backward()\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "med_inf,  std_inf  = _cuda_timer(_fwd_step,   warm=100, iters=1000)\n",
        "med_train, std_train = _cuda_timer(_train_step, warm=50,  iters=300)\n",
        "\n",
        "print(f\"Inference  : {med_inf:.2f} ± {std_inf:.2f} ms\")\n",
        "print(f\"Train step : {med_train:.2f} ± {std_train:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "xNCRSMckc8Iz",
        "outputId": "cf27fee9-0f68-4965-c871-2db50f821f09"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference  : 17.55 ± 11.15 ms\n",
            "Train step : 57.21 ± 24.53 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Kernel Optimizations"
      ],
      "metadata": {
        "id": "0IAq0w4CmdD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import triton, triton.language as tl\n",
        "import torch, types, math, time, gc\n",
        "from contextlib import nullcontext\n",
        "\n",
        "@triton.jit\n",
        "def _tanh(x):\n",
        "    return 2.0 * tl.sigmoid(2.0 * x) - 1.0\n",
        "\n",
        "@triton.jit\n",
        "def _fast_gelu(x):\n",
        "    return 0.5 * x * (1.0 + _tanh(0.79788456 * (x + 0.044715 * x * x * x)))\n",
        "\n",
        "\n",
        "@triton.jit\n",
        "def fbgemm_kernel(X, W, B, Y,\n",
        "                  M, N, K,\n",
        "                  BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr,\n",
        "                  BLOCK_K: tl.constexpr):\n",
        "    pid_m = tl.program_id(0)          # Process ID along M dimension\n",
        "    pid_n = tl.program_id(1)          # along N dimension\n",
        "\n",
        "    offs_m = pid_m * BLOCK_M + tl.arange(0, BLOCK_M)\n",
        "    offs_n = pid_n * BLOCK_N + tl.arange(0, BLOCK_N)\n",
        "    offs_k = tl.arange(0, BLOCK_K)\n",
        "\n",
        "    X_ptrs = X + (offs_m[:, None] * K + offs_k[None, :])\n",
        "    W_ptrs = W + (offs_n[None, :] * K + offs_k[:, None])\n",
        "    acc = tl.zeros((BLOCK_M, BLOCK_N), tl.float32)\n",
        "\n",
        "    for k in range(0, K, BLOCK_K):\n",
        "        x = tl.load(X_ptrs + k,\n",
        "                    mask=(offs_m[:, None] < M) & (offs_k[None, :] < K),\n",
        "                    other=0.0)\n",
        "        w = tl.load(W_ptrs + k,\n",
        "                    mask=(offs_n[None, :] < N) & (offs_k[:, None] < K),\n",
        "                    other=0.0)\n",
        "        acc += tl.dot(x, w)\n",
        "\n",
        "    # add bias\n",
        "    bias = tl.load(B + offs_n, mask=offs_n < N, other=0.0)\n",
        "    acc = acc + bias[None, :]\n",
        "\n",
        "    # fast-GELU in fp16 for storage\n",
        "    acc_f16 = _fast_gelu(acc)\n",
        "    acc_f16 = acc.to(tl.float16)\n",
        "\n",
        "    tl.store(Y + (offs_m[:, None] * N + offs_n[None, :]),\n",
        "             acc_f16,\n",
        "             mask=(offs_m[:, None] < M) & (offs_n[None, :] < N))\n",
        "\n",
        "def fused_bias_gelu_triton(x: torch.Tensor,\n",
        "                            w: torch.Tensor,\n",
        "                            b: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"x:[M,K]  w:[N,K]  b:[N]  -> y:[M,N] (all fp16)\"\"\"\n",
        "    M, K = x.shape\n",
        "    N = w.shape[0]\n",
        "    y = torch.empty((M, N), dtype=x.dtype, device=x.device)\n",
        "\n",
        "    BLOCK_M, BLOCK_N, BLOCK_K = 128, 128, 32\n",
        "    grid = (triton.cdiv(M, BLOCK_M),\n",
        "            triton.cdiv(N, BLOCK_N))\n",
        "\n",
        "    fbgemm_kernel[grid](\n",
        "        x, w, b, y,\n",
        "        M, N, K,\n",
        "        BLOCK_M=BLOCK_M, BLOCK_N=BLOCK_N, BLOCK_K=BLOCK_K,\n",
        "        num_warps=4, num_stages=2\n",
        "    )\n",
        "    return y"
      ],
      "metadata": {
        "id": "ax3RmPodpJew"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Autograd wrapper (fused fwd, stock bwd)\n",
        "class TritonFBGGEMM(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, w, b):\n",
        "        y = fused_bias_gelu_triton(x, w, b)\n",
        "        ctx.save_for_backward(x, w, b, y)\n",
        "        return y\n",
        "    @staticmethod\n",
        "    def backward(ctx, dy):\n",
        "        x, w, b, y = ctx.saved_tensors\n",
        "        # GELU backward (approx.)\n",
        "        pre = y\n",
        "        tanh_out = torch.tanh(0.79788456 *\n",
        "                              (pre + 0.044715 * pre * pre * pre))\n",
        "        gelu_grad = 0.5 * (1.0 + tanh_out) + \\\n",
        "                    0.5 * pre * (1 - tanh_out * tanh_out) * \\\n",
        "                    (0.79788456 + 0.134145 * pre * pre)\n",
        "        dy_pre = dy * gelu_grad\n",
        "\n",
        "        dx = torch.matmul(dy_pre, w)                 # [M,K]\n",
        "        dw = torch.matmul(dy_pre.t(), x)         # [N,K]\n",
        "        db = dy_pre.sum(dim=0)\n",
        "        return dx, dw, db"
      ],
      "metadata": {
        "id": "k02EzCCepw6j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Monkey-patch every feed-forward “intermediate dense” layer\n",
        "def patch_ffn(block):\n",
        "    lin1, lin2 = block.intermediate.dense, block.output.dense\n",
        "    W1 = lin1.weight.contiguous().half()   # [N,K] for row-major load\n",
        "    B1 = lin1.bias.contiguous().half()\n",
        "    def fused_forward(self, hidden_states):\n",
        "        B, S, K = hidden_states.shape\n",
        "        x = hidden_states.reshape(-1, K)                # [M,K]\n",
        "        y = TritonFBGGEMM.apply(x, W1, B1)              # fused op\n",
        "        return y.view(B, S, -1)\n",
        "    block.intermediate.forward = types.MethodType(fused_forward,\n",
        "                                                  block.intermediate)"
      ],
      "metadata": {
        "id": "neYvKg5xpzr5"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for blk in model.bert.encoder.layer:\n",
        "    patch_ffn(blk)\n",
        "print(\"Patched FFN (intermediate dense) with Triton fused kernel\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "mXfuf75ip2Gj",
        "outputId": "03987815-3060-41cc-95ea-5e20c6daa8c2"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Patched FFN (intermediate dense) with Triton fused kernel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.inference_mode()\n",
        "def _fwd_step():\n",
        "    _ = model(**batch)\n",
        "\n",
        "def _train_step():\n",
        "    loss = model(**batch, labels=batch[\"input_ids\"]).loss\n",
        "    loss.backward()\n",
        "    model.zero_grad(set_to_none=True)\n",
        "\n",
        "med_inf,  std_inf  = _cuda_timer(_fwd_step,   warm=100, iters=1000)\n",
        "med_train, std_train = _cuda_timer(_train_step, warm=50,  iters=300)\n",
        "\n",
        "print(f\"Inference  : {med_inf:.2f} ± {std_inf:.2f} ms\")\n",
        "print(f\"Train step : {med_train:.2f} ± {std_train:.2f} ms\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "om15sLmlqJnl",
        "outputId": "2fbfacfe-af42-4c88-c8db-05212c916b5f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inference  : 11.65 ± 2.25 ms\n",
            "Train step : 35.36 ± 7.23 ms\n"
          ]
        }
      ]
    }
  ]
}